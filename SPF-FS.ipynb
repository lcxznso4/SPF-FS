import os
import numpy as np
import pandas as pd
import cvxpy as cp
from numpy.linalg import eigh, matrix_rank
from scipy.sparse import csr_matrix, lil_matrix
import scanpy as sc
import anndata
from sklearn.mixture import GaussianMixture
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

EPS = 1e-10

############################
# (A) 数据 & 第一阶段筛选
############################

def preprocess_data(file, min_samples=3):
    """
    1) 过滤在 min_samples 个样本中表达 > 0 的基因
    2) log(1+x) 转换
    返回 X: shape=(d,n), df_normalized: DataFrame(行=样本, 列=基因)
    """
    df = pd.read_csv(file, index_col=0)
    # 过滤
    df_filtered = df.loc[:, (df > 0).sum(axis=0) >= min_samples]
    # 对数归一化
    df_normalized = np.log1p(df_filtered)
    # 转置 => X.shape=(d,n)
    X = df_normalized.values.T
    return X, df_normalized

def compute_variance_score(X):
    """
    简易方差打分
    X: (d,n)
    返回每个基因(行)的方差
    """
    return np.var(X, axis=1)

############################
# (B) 多指标打分 + Leiden => 基结果
############################

def compute_hvg_scores(X_subset):
    """
    基于方差的 HVG 打分
    X_subset: (d, n_sub)
    """
    var_ = np.var(X_subset, axis=1)
    scores = (var_ - var_.min()) / (var_.max() - var_.min() + EPS)
    return scores

def compute_fano_scores(X_subset):
    """
    Fano 因子打分
    """
    mean_ = np.mean(X_subset, axis=1) + EPS
    var_  = np.var(X_subset, axis=1)
    fano  = var_ / mean_
    scores= (fano - fano.min()) / (fano.max() - fano.min() + EPS)
    return scores

def compute_gmm_scores(X_subset):
    """
    用 GaussianMixture(n=1 vs n=2) 的 BIC差计算打分
    """
    d,_ = X_subset.shape
    gmm_score= np.zeros(d)
    for i in range(d):
        data_i = X_subset[i,:].reshape(-1,1)

        gm1= GaussianMixture(n_components=1, random_state=0).fit(data_i)
        gm2= GaussianMixture(n_components=2, random_state=0).fit(data_i)
        bic1= gm1.bic(data_i)
        bic2= gm2.bic(data_i)
        diff= bic1 - bic2
        # 若差值>0 表示 n=2 成分模型更优 => 打分更高
        gmm_score[i]= max(diff, 0.0)

    # 归一化
    gmm_score = (gmm_score - gmm_score.min()) / (gmm_score.max() - gmm_score.min() + EPS)
    return gmm_score

def spearman_distance_matrix(X):
    """
    计算 Spearman 距离矩阵, X:(d,n) => ranks => 1-corr => dist
    """
    X_t = X.T   # shape=(n,d)
    # 对每行做排序 => rank
    ranks = np.argsort(np.argsort(X_t, axis=1), axis=1).astype(float)
    # 去中心化
    rc = ranks - ranks.mean(axis=1, keepdims=True)
    cov= rc @ rc.T
    norm_ = np.linalg.norm(rc, axis=1, keepdims=True)
    corr = cov/(norm_ * norm_.T + EPS)
    dist = 1 - corr
    return dist

def construct_knn_graph(dist, k=10):
    """
    基于距离矩阵构造 kNN 图, 对称化
    dist: (n,n)
    返回 adj_sym: (n,n)
    """
    n= dist.shape[0]
    adj= np.zeros((n,n), dtype=float)
    for i in range(n):
        idx_sorted= np.argsort(dist[i,:])
        neighbors= idx_sorted[1:k+1]
        adj[i, neighbors] = 1
    adj_sym= np.maximum(adj, adj.T)
    return adj_sym

def leiden_clustering(adj, resolution=1.0):
    """
    在给定邻接矩阵上做 Leiden 聚类
    """
    adata= anndata.AnnData(np.zeros((adj.shape[0],1)))
    A_csr= csr_matrix(adj)
    adata.obsp["connectivities"] = A_csr
    adata.uns["neighbors"]={}
    adata.uns["neighbors"]["connectivities_key"]="connectivities"
    sc.tl.leiden(adata, resolution=resolution, key_added='leiden')
    labels= adata.obs['leiden'].astype(int).values
    return labels

def compute_base_result_leiden(i, X, select_ratio=0.5, k=10, resolution=1.0):
    """
    1) 随机抽取 50% 样本 => HVG, Fano, GMM 得分合并 => v^(k)
    2) 按得分选 top 50% 基因 => Spearman => kNN => Leiden => S^(k)
    """
    d,n= X.shape
    idx_sub= np.random.choice(n, size=int(0.5*n), replace=False)
    X_sub= X[:, idx_sub]

    # 三种打分
    s1= compute_hvg_scores(X_sub)
    s2= compute_fano_scores(X_sub)
    s3= compute_gmm_scores(X_sub)
    combined= (s1+s2+s3)/3.0

    # 选前 50% 基因
    num_select= int(d * select_ratio)
    idx_sel= np.argsort(combined)[-num_select:]
    X_sel= X[idx_sel, :]

    dist= spearman_distance_matrix(X_sel)
    adj= construct_knn_graph(dist, k=k)
    labels= leiden_clustering(adj, resolution=resolution)

    S_k= lil_matrix((n,n))
    n_label= np.max(labels) + 1
    for c in range(n_label):
        members= np.where(labels==c)[0]
        S_k[np.ix_(members, members)] = 1

    return combined, S_k.tocsr()

def generate_base_results_leiden(X, m=10, select_ratio=0.5, k=10, resolution=1.0):
    """
    生成 m 个基学习器的 (scores, connectivity) 对
    """
    base_scores=[]
    base_connectivity=[]
    for i in range(m):
        scr, conn= compute_base_result_leiden(i, X, select_ratio, k=k, resolution=resolution)
        base_scores.append(scr)
        base_connectivity.append(conn)
    return base_scores, base_connectivity

############################
# (B2) GO BP 富集后 p值 => 先验 w
############################

def run_gobp_enrichment(gene_list):
    
    import gseapy

    # 假设 c5.go.bp.v2024.1.Hs.symbols.gmt 与本脚本在同一目录
    # 如果在其它路径, 请自行修改下方路径
    local_gmt_path = "data/c5.go.bp.v2024.1.Hs.symbols.gmt"

    enr = gseapy.enrichr(
        gene_list = gene_list,         # 基因列表
        gene_sets = local_gmt_path,    # 指向本地 GMT 文件
        outdir    = None,             # 不保存到硬盘, 仅保留内存
        cutoff    = 1.0               # p-value 截断
        # enrichr_url=None,           # 一般不用也行, 只要 gene_sets=本地文件就不会远程
    )
    enr.run()
    return enr.res2d

def define_additive_prior_gobp(gene_list, p_cut=0.05):
    """
    对 gene_list 做 GO:BP 富集, 将最小 P-value 映射到 [0.2,0.8]
    """
    res_df= run_gobp_enrichment(gene_list)
    gene2p= {g:1.0 for g in gene_list}
    if len(res_df)>0:
        for _,row in res_df.iterrows():
            p= row["P-value"]
            genes_in_term= row["Genes"].replace(" ","").split(";")
            for gg in genes_in_term:
                gene2p[gg] = min(gene2p.get(gg,1.0), p)

    w=[]
    for g in gene_list:
        p= gene2p[g]
        if p> p_cut:
            w.append(0.2)
        else:
            ratio= (p_cut - p)/ p_cut
            val= 0.2 + 0.6* ratio
            w.append(val)
    return np.array(w)

def generate_base_priors_gobp(X, base_scores, gene_names, p_cut=0.05, ratio_for_enrich=0.5):
    """
    对每个基结果 v^(k):
      1) 选出前 (ratio_for_enrich*d) 基因
      2) GO富集 => 映射成 w_sub
      3) 映射回 d维, 其余默认填0.2
    """
    m = len(base_scores)
    d = len(gene_names)
    base_priors = []

    for k in range(m):
        scr = base_scores[k]
        num_sel = int(d * ratio_for_enrich)
        idx_sel = np.argsort(scr)[-num_sel:]

        gene_sub_list = gene_names[idx_sel].tolist()
        w_sub = define_additive_prior_gobp(gene_sub_list, p_cut=p_cut)

        w_all = np.full(d, 0.2, dtype=float)
        for i_sub, g_idx in enumerate(idx_sel):
            w_all[g_idx] = w_sub[i_sub]
        base_priors.append(w_all)

    return np.array(base_priors)

############################
# (C) 自进学习 & 其他更新函数
############################

def update_S(W, base_connectivity, P, X, v, Y, beta, rho):
    """
    更新共识邻接矩阵 S 
    """
    m=len(base_connectivity)
    n=X.shape[1]
    dv= np.sqrt(v+EPS)
    D_v= np.diag(dv)
    weighted_X= D_v@X
    Z= P@weighted_X
    Z2= np.sum(Z**2, axis=0)
    B= np.add.outer(Z2,Z2) - 2*(Z.T@Z)

    Y2= np.sum(Y**2, axis=1)
    C= np.add.outer(Y2,Y2) - 2*(Y@Y.T)

    beta_sum= np.sum(beta**2)
    S_acc= np.zeros((n,n))
    for k in range(m):
        S_acc += (beta[k]**2)* base_connectivity[k].toarray()

    W_plus= 2*W*beta_sum + EPS
    numerator= S_acc - B + rho*C
    S_new= numerator / W_plus
    S_new= np.clip(S_new,0,1)

    from scipy.sparse import csr_matrix
    return csr_matrix(S_new)

def update_W(S, base_connectivity, beta, lam):
    """
    更新自步学习权重矩阵 W
    W_(i,j) = min{1, lambda / (2*∑(beta_k^2 * (S - S^(k))^2)) }
    """
    m=len(base_connectivity)
    S_d=S.toarray()
    A= np.zeros_like(S_d)
    for k in range(m):
        diff= S_d - base_connectivity[k].toarray()
        A+= (beta[k]**2)*(diff**2)
    W= np.minimum(lam / (2*(A+EPS)), 1)
    return W

def update_alpha(v, base_scores):
    """
    α_k ∝ 1 / (‖v - v^(k)‖^2 + eps)
    """
    m=len(base_scores)
    arr= np.zeros(m)
    for k in range(m):
        dist= np.linalg.norm(v - base_scores[k])**2
        arr[k]= 1.0/(dist + EPS)
    alpha= arr/(arr.sum() + EPS)
    return alpha

def update_beta(S, base_connectivity, W):
    """
    β_k ∝ 1 / (‖W⊙(S - S^(k))‖_F^2 + eps)
    """
    m=len(base_connectivity)
    S_d= S.toarray()
    arr= np.zeros(m)
    for k in range(m):
        diff= W*(S_d - base_connectivity[k].toarray())
        val= np.linalg.norm(diff,'fro')**2
        arr[k]= 1.0/(val + EPS)
    beta= arr/(arr.sum() + EPS)
    return beta

############################
# (C2) 特征向量合并
############################

def update_u_for_each_base(base_scores, base_priors, gamma):
    """
    计算 u^(k) = v^(k) + gamma * w^(k), 并做 clip+归一化
    """
    m= len(base_scores)
    u_list= []
    for k in range(m):
        v_k= base_scores[k]
        w_k= base_priors[k]
        u_k= v_k + gamma*w_k

        # clip & 归一化
        u_k= np.clip(u_k, 0, None)
        sum_uk= u_k.sum() + EPS
        u_k/= sum_uk
        u_list.append(u_k)
    return u_list

def update_v_with_u(X, u_list, alpha):
    """
    在单纯形约束下求解  min_v Σ( alpha_k^2 * ||v - u^(k)||^2 )
    """
    import cvxpy as cp
    m= len(u_list)
    d= X.shape[0]
    v_var= cp.Variable(d, nonneg=True)

    objective= 0
    for k in range(m):
        objective += alpha[k]**2 * cp.sum_squares(v_var - u_list[k])

    constraints= [cp.sum(v_var)==1, v_var<=1]
    prob= cp.Problem(cp.Minimize(objective), constraints)
    prob.solve(solver=cp.ECOS, verbose=False)

    if v_var.value is None:
        v_new= np.ones(d)/d
    else:
        v_new= np.maximum(v_var.value, 0)
        sum_v= v_new.sum() + EPS
        v_new/= sum_v
    return v_new

def update_P(X,v,S,n_clusters):
  
    d,n= X.shape
    dv= np.sqrt(v+EPS)
    D_v= np.diag(dv)
    S_d= S.toarray()
    D= np.diag(S_d.sum(axis=1))
    L= D-S_d
    M= D_v@X@L@(X.T)@D_v
    eigvals,eigvecs= eigh(M)
    idx= np.argsort(eigvals)
    P= eigvecs[:, idx[:n_clusters]].T
    return P

def update_Y(S,n_clusters):

    S_d= S.toarray()
    D= np.diag(S_d.sum(axis=1))
    L= D-S_d
    eigvals,eigvecs= eigh(L)
    idx= np.argsort(eigvals)
    Y= eigvecs[:, idx[:n_clusters]]
    return Y, L

############################
# (C4) 簇间分离: update_P_enhanced / update_Y_enhanced
############################

def update_P_enhanced(X, v, S, Y, eta, n_clusters):
    """
    更新 P, 考虑 -eta * ||y_i - y_j||^2 => W_tilde = S - eta*distY, clip>=0 => L_tilde
    """
    S_d = S.toarray()
    n = S_d.shape[0]

    # distY[i,j] = ||y_i - y_j||^2
    Y2 = np.sum(Y**2, axis=1, keepdims=True)
    distY = Y2 + Y2.T - 2*(Y @ Y.T)

    W_tilde = S_d - eta * distY
    W_tilde = np.clip(W_tilde, 0, None)

    D_tilde = np.diag(W_tilde.sum(axis=1))
    L_tilde = D_tilde - W_tilde

    d,_ = X.shape
    dv = np.sqrt(v + EPS)
    D_v = np.diag(dv)
    M = D_v @ X @ L_tilde @ X.T @ D_v

    eigvals, eigvecs= eigh(M)
    idx= np.argsort(eigvals)
    P= eigvecs[:, idx[:n_clusters]].T
    return P

def update_Y_enhanced(S, P, X, v, eta, n_clusters):
    """
    更新 Y, 考虑 -eta * ||Z_i - Z_j||^2 => W_tilde = S - eta*distZ, distZ=||Z_i - Z_j||^2
    """
    S_d= S.toarray()
    n= S_d.shape[0]

    dv= np.sqrt(v+EPS)
    D_v= np.diag(dv)
    ZX = D_v @ X    # shape=(d,n)
    Z  = P @ ZX     # shape=(c,n), Z[:,j] = x_j的映射

    Z2 = np.sum(Z**2, axis=0, keepdims=True)
    distZ = Z2 + Z2.T - 2*(Z.T @ Z)

    W_tilde = S_d - eta*distZ
    W_tilde = np.clip(W_tilde, 0, None)

    D_tilde = np.diag(W_tilde.sum(axis=1))
    L_tilde = D_tilde - W_tilde

    eigvals, eigvecs = eigh(L_tilde)
    idx = np.argsort(eigvals)
    Y= eigvecs[:, idx[:n_clusters]]

    return Y, L_tilde

############################
# (C5) 逐步增大 λ 的主循环
############################

def BLFSE_intraBio_with_localU(
    X,
    base_scores,
    base_priors,
    base_connectivity,
    gamma=0.5,
    n_clusters=5,
    max_iter=10,
    eta=0.1,
    lambda_init=0.2,     # <-- 初始 λ
    lambda_growth=0.3,   # <-- 每轮迭代后以该比例递增
    rho=1.0
):
    """
    基于双层融合 + 先验 + 自步学习 + 簇间分离 的算法主循环.
    每次迭代后 lambda *= (1 + lambda_growth), 使更多样本对被纳入.
    """
    from scipy.sparse import csr_matrix

    m= len(base_scores)
    d,n= X.shape

    # 1) 初始化 v => base_scores平均
    v= np.mean(np.array(base_scores), axis=0)
    v/= (v.sum() + EPS)

    # 初始化共识邻接 S
    S_init= np.mean([bc.toarray() for bc in base_connectivity], axis=0)
    S= csr_matrix(S_init)

    # 初始化 alpha, beta
    alpha= np.ones(m)/m
    beta= np.ones(m)/m

    lam= lambda_init  # 初始 λ

    # 给 Y 一个初始值
    Y, L = update_Y(S, n_clusters)

    for it in range(max_iter):
        logging.info(f"=== Iteration {it+1}/{max_iter}, lambda={lam:.4f} ===")

        # (1) 更新 W (自步学习)
        W= update_W(S, base_connectivity, beta, lam)

        # (2) 更新 P (增强版, 簇间分离)
        P= update_P_enhanced(X, v, S, Y, eta, n_clusters)

        # (3) 更新 Y (增强版, 簇间分离)
        Y, L_tilde = update_Y_enhanced(S, P, X, v, eta, n_clusters)

        # (4) 更新 S
        S= update_S(W, base_connectivity, P, X, v, Y, beta, rho)

        # (5) 更新特征向量 v: u^(k)= v^(k)+gamma*w^(k), 再投影
        u_list= update_u_for_each_base(base_scores, base_priors, gamma=gamma)
        v= update_v_with_u(X, u_list, alpha)

        # (6) 更新 alpha, beta
        alpha= update_alpha(v, base_scores)
        beta= update_beta(S, base_connectivity, W)

        # (7) 逐步增大 lambda
        lam *= (1.0 + lambda_growth)

        # 若要近似 rank(D-S)=n-c, 可自适应 rho:
        # r= matrix_rank(L_tilde)
        # if r> n-n_clusters: rho *= 2
        # elif r< n-n_clusters: rho /= 2

    return v, S, P, Y, alpha, beta

############################
# (D) main 函数
############################

def main():
    # 你可以替换为自己的数据文件
    data_file= "./data/pollen_counts.csv"

    X_full, df_full= preprocess_data(data_file, min_samples=3)
    d_full, n= X_full.shape
    logging.info(f"Data => genes={d_full}, samples={n}")

    # 1) 根据方差筛选 top2000
    var_all= compute_variance_score(X_full)
    topN=2000
    idx_top= np.argsort(var_all)[-topN:]
    X_2000= X_full[idx_top,:]
    gene_names_2000= df_full.columns[idx_top]
    logging.info(f"Selected top {topN} genes by variance")

    # 2) 生成基结果 (HVG,Fano,GMM+Leiden)
    m=10
    base_scores, base_conn= generate_base_results_leiden(
        X_2000, m=m, select_ratio=0.5, k=10, resolution=1.0
    )

    # 3) 生成 GO 先验 (离线模式)
    #   - p_cut 可设小一些, 强化对显著功能条目的聚焦
    base_priors= generate_base_priors_gobp(
        X_2000,
        base_scores,
        gene_names_2000,
        p_cut=1e-5,       # 比较严格的阈值
        ratio_for_enrich=0.5
    )
    logging.info("Generated base_priors from local GO BP enrichment (offline).")

    # 4) 调用改进版 BLFSE, 设定 lambda 初始值和增长率
    v_final, S_final, P_final, Y_final, alpha_final, beta_final= BLFSE_intraBio_with_localU(
        X_2000,
        base_scores,
        base_priors,
        base_conn,
        gamma=1,
        n_clusters=11,
        max_iter=10,
        eta=0.2,
        lambda_init=0.2,
        lambda_growth=0.3
    )

    # 5) 导出选出的基因
    outdir = "./spf-只使用HVG"
    os.makedirs(outdir, exist_ok=True)

    for topN_final in [400]:
        if topN_final > X_2000.shape[0]:
            logging.warning(f"topN_final={topN_final} > #genes({X_2000.shape[0]}) => skip.")
            continue

        idx_sel = np.argsort(v_final)[-topN_final:]
        sel_genes = gene_names_2000[idx_sel]
        logging.info(f"Top {topN_final} genes => {sel_genes[:10].tolist()} ...")

        X_sel = X_2000[idx_sel, :].T
        sel_df = pd.DataFrame(X_sel, columns=sel_genes, index=df_full.index)

        outname = f"{outdir}/pollen1_methodB_GOBP_top{topN_final}.csv"
        sel_df.to_csv(outname)
        logging.info(f"Saved => {outname}")

if __name__ == "__main__":
    main()
